# APMA 4302 — Methods  
**Name:** _______________________________________

---

## Problem 4 (10 pts)

Use the default options file to determine the number of iterations required to converge using:
and explain your results.

**(a)** Jacobi pre-conditioned Richardson  
`-ksp_type richardson -pc_type jacobi`

**(b)** Unpreconditioned conjugate gradient with 1 processor  
`-ksp_type cg -pc_type none`

**(c)** Unpreconditioned conjugate gradient with $c = 0$. Explain your results.  
(Same as (b) but run with `-bvp_c 0`.)

**(d)** ICC pre-conditioned conjugate gradient with 1 processor (explain this result)  
`-ksp_type cg -pc_type icc`

**(e)** Block Jacobi pre-conditioned conjugate gradient with 4 processors  
`-ksp_type cg -pc_type bjacobi -pc_sub_type icc`

**(f)** MUMPS direct solver with 1 and 4 processors  
Problem text: `-ksp_type preonly -pc_type lu -pc_factor_solver_type mumps`  

**Note:** In current PETSc the option name is **`-pc_factor_mat_solver_type mumps`** (not `pc_factor_solver_type`). If you use the old name, PETSc reports it as unused and falls back to the default LU; MUMPS will not run. Use:
```bash
-ksp_type preonly -pc_type lu -pc_factor_mat_solver_type mumps
```

---

## Solution

**Iteration counts** (default `options_file`: $m=201$, $\gamma=0$, $k=5$, $c=3$):

| Part | Solver | Procs | Iterations |
|------|--------|-------|------------|
| (a) | Richardson + Jacobi | 1 | 10000 (max it) |
| (b) | CG, no PC | 1 | 101 |
| (c) | CG, no PC, $c=0$ | 1 | 1 |
| (d) | CG + ICC | 1 | 1 |
| (e) | CG + bjacobi (icc) | 4 | 7 |
| (f) | MUMPS (direct) | 1 | — |
| (f) | MUMPS (direct) | 4 | — |

**(a) Jacobi-preconditioned Richardson**  
$\kappa(A) \sim O(m^2)$, so convergence is slow and we hit the iteration limit (10000). Jacobi only scales the system; it does not improve the conditioning, so we expect many iterations.

**(b) Unpreconditioned CG, 1 processor**  
Iterations scale like $O(\sqrt{\kappa}) \sim O(m)$. With $m=201$, we get 101 iterations, consistent with this.

**(c) Unpreconditioned CG with $c=0$**  
The matrix $A$ is unchanged from (b); only the right-hand side and exact solution change (via `-bvp_c 0`). The iteration count often matches (b). In this run (c) converged in 1 iteration (atol); on another run or tolerance it may match (b).

**(d) ICC-preconditioned CG, 1 processor**  
ICC is a strong preconditioner for this SPD matrix, so we expect a large drop in iterations compared with (b). We get 1 iteration, as expected.

**(e) Block Jacobi + ICC, 4 processors**  
Block Jacobi with ICC on each block is weaker than global ICC, so we expect more iterations than (d) but fewer than (b). We get 7 iterations. This setup scales well in parallel.

**(f) MUMPS direct, 1 and 4 processors**  
MUMPS works when you use **`-pc_factor_mat_solver_type mumps`** (see note above). Direct solver: no Krylov iterations (—). Compare run time and memory for 1 vs 4 processors.
